{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7386965c-23e8-4cc7-9e14-27c6ed714a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SUPPLY CHAIN RISK PREDICTION USING GRAPH NEURAL NETWORKS\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SUPPLY CHAIN RISK PREDICTION USING GRAPH NEURAL NETWORKS\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f8e8503-75c0-4611-a0e5-0d7df8670a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 1] Loading Dataset...\n",
      "\n",
      "Dataset Shape: (10000, 53)\n",
      "\n",
      "First 5 rows:\n",
      "       Type  Days for shipping (real)  Days for shipment (scheduled)  \\\n",
      "0  TRANSFER                         5                              4   \n",
      "1   PAYMENT                         2                              1   \n",
      "2  TRANSFER                         2                              4   \n",
      "3  TRANSFER                         5                              4   \n",
      "4     DEBIT                         2                              4   \n",
      "\n",
      "   Benefit per order  Sales per customer   Delivery Status  \\\n",
      "0          11.090000          175.990005     Late delivery   \n",
      "1           9.800000          245.000000     Late delivery   \n",
      "2         117.550003          244.899994  Advance shipping   \n",
      "3         118.430000          251.979996     Late delivery   \n",
      "4         -21.590000          107.970001  Advance shipping   \n",
      "\n",
      "   Late_delivery_risk  Category Id         Category Name Customer City  ...  \\\n",
      "0                   1           48          Water Sports        Caguas  ...   \n",
      "1                   1           24       Women's Apparel    Florissant  ...   \n",
      "2                   0           46  Indoor/Outdoor Games    Carmichael  ...   \n",
      "3                   1           43      Camping & Hiking          Troy  ...   \n",
      "4                   0           29         Shop By Sport        Caguas  ...   \n",
      "\n",
      "  Order Zipcode Product Card Id Product Category Id  Product Description  \\\n",
      "0       90036.0            1073                  48                  NaN   \n",
      "1           NaN             502                  24                  NaN   \n",
      "2           NaN            1014                  46                  NaN   \n",
      "3           NaN             957                  43                  NaN   \n",
      "4           NaN             627                  29                  NaN   \n",
      "\n",
      "                                       Product Image  \\\n",
      "0  http://images.acmesports.sports/Pelican+Sunstr...   \n",
      "1  http://images.acmesports.sports/Nike+Men%27s+D...   \n",
      "2  http://images.acmesports.sports/O%27Brien+Men%...   \n",
      "3  http://images.acmesports.sports/Diamondback+Wo...   \n",
      "4  http://images.acmesports.sports/Under+Armour+G...   \n",
      "\n",
      "                                    Product Name Product Price Product Status  \\\n",
      "0                    Pelican Sunstream 100 Kayak    199.990005              0   \n",
      "1           Nike Men's Dri-FIT Victory Golf Polo     50.000000              0   \n",
      "2               O'Brien Men's Neoprene Life Vest     49.980000              0   \n",
      "3  Diamondback Women's Serene Classic Comfort Bi    299.980011              0   \n",
      "4  Under Armour Girls' Toddler Spine Surge Runni     39.990002              0   \n",
      "\n",
      "  shipping date (DateOrders)   Shipping Mode  \n",
      "0             4/6/2016 21:05  Standard Class  \n",
      "1            6/11/2017 18:43     First Class  \n",
      "2             1/4/2015 14:32  Standard Class  \n",
      "3             1/15/2017 1:46  Standard Class  \n",
      "4              4/8/2017 7:15  Standard Class  \n",
      "\n",
      "[5 rows x 53 columns]\n",
      "\n",
      "Column Names: ['Type', 'Days for shipping (real)', 'Days for shipment (scheduled)', 'Benefit per order', 'Sales per customer', 'Delivery Status', 'Late_delivery_risk', 'Category Id', 'Category Name', 'Customer City', 'Customer Country', 'Customer Email', 'Customer Fname', 'Customer Id', 'Customer Lname', 'Customer Password', 'Customer Segment', 'Customer State', 'Customer Street', 'Customer Zipcode', 'Department Id', 'Department Name', 'Latitude', 'Longitude', 'Market', 'Order City', 'Order Country', 'Order Customer Id', 'order date (DateOrders)', 'Order Id', 'Order Item Cardprod Id', 'Order Item Discount', 'Order Item Discount Rate', 'Order Item Id', 'Order Item Product Price', 'Order Item Profit Ratio', 'Order Item Quantity', 'Sales', 'Order Item Total', 'Order Profit Per Order', 'Order Region', 'Order State', 'Order Status', 'Order Zipcode', 'Product Card Id', 'Product Category Id', 'Product Description', 'Product Image', 'Product Name', 'Product Price', 'Product Status', 'shipping date (DateOrders)', 'Shipping Mode']\n",
      "\n",
      "Data Types:\n",
      "Type                              object\n",
      "Days for shipping (real)           int64\n",
      "Days for shipment (scheduled)      int64\n",
      "Benefit per order                float64\n",
      "Sales per customer               float64\n",
      "Delivery Status                   object\n",
      "Late_delivery_risk                 int64\n",
      "Category Id                        int64\n",
      "Category Name                     object\n",
      "Customer City                     object\n",
      "Customer Country                  object\n",
      "Customer Email                    object\n",
      "Customer Fname                    object\n",
      "Customer Id                        int64\n",
      "Customer Lname                    object\n",
      "Customer Password                 object\n",
      "Customer Segment                  object\n",
      "Customer State                    object\n",
      "Customer Street                   object\n",
      "Customer Zipcode                   int64\n",
      "Department Id                      int64\n",
      "Department Name                   object\n",
      "Latitude                         float64\n",
      "Longitude                        float64\n",
      "Market                            object\n",
      "Order City                        object\n",
      "Order Country                     object\n",
      "Order Customer Id                  int64\n",
      "order date (DateOrders)           object\n",
      "Order Id                           int64\n",
      "Order Item Cardprod Id             int64\n",
      "Order Item Discount              float64\n",
      "Order Item Discount Rate         float64\n",
      "Order Item Id                      int64\n",
      "Order Item Product Price         float64\n",
      "Order Item Profit Ratio          float64\n",
      "Order Item Quantity                int64\n",
      "Sales                            float64\n",
      "Order Item Total                 float64\n",
      "Order Profit Per Order           float64\n",
      "Order Region                      object\n",
      "Order State                       object\n",
      "Order Status                      object\n",
      "Order Zipcode                    float64\n",
      "Product Card Id                    int64\n",
      "Product Category Id                int64\n",
      "Product Description              float64\n",
      "Product Image                     object\n",
      "Product Name                      object\n",
      "Product Price                    float64\n",
      "Product Status                     int64\n",
      "shipping date (DateOrders)        object\n",
      "Shipping Mode                     object\n",
      "dtype: object\n",
      "\n",
      "  Missing Values BEFORE Cleaning:\n",
      "Order Zipcode           8619\n",
      "Product Description    10000\n",
      "dtype: int64\n",
      "Total missing values: 18619\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================\n",
    "# STEP 1: LOAD AND EXPLORE DATA\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 1] Loading Dataset...\")\n",
    "df = pd.read_csv('sampled_10000.csv')\n",
    "\n",
    "print(f\"\\nDataset Shape: {df.shape}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\nColumn Names: {df.columns.tolist()}\")\n",
    "print(f\"\\nData Types:\\n{df.dtypes}\")\n",
    "print(f\"\\n  Missing Values BEFORE Cleaning:\")\n",
    "missing_summary = df.isnull().sum()\n",
    "print(missing_summary[missing_summary > 0])\n",
    "print(f\"Total missing values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "020b94f2-de01-4d2d-aba6-36040111cd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "[STEP 2] Data Cleaning & Preprocessing...\n",
      "======================================================================\n",
      "\n",
      " Handling Missing Values in Critical Columns...\n",
      "  Order City: 0 missing (0.00%)\n",
      "  Customer City: 0 missing (0.00%)\n",
      "  Late_delivery_risk: 0 missing (0.00%)\n",
      "\n",
      " Dropping rows with missing critical values...\n",
      "  Rows dropped: 0\n",
      "  Remaining rows: 7115\n",
      "\n",
      " Cleaning city names...\n",
      "  After cleaning empty strings: 7115 rows\n",
      "\n",
      "  Validating Late_delivery_risk column...\n",
      "  Unique values: [1 0]\n",
      "  Valid risk values: {1: 4189, 0: 2926}\n",
      "\n",
      " Handling Shipping Mode...\n",
      "  Missing Shipping Mode: 0\n",
      "\n",
      " Handling duplicate routes...\n",
      "  Duplicate routes removed: 0\n",
      "  Remaining unique routes: 7115\n",
      "\n",
      " Data Cleaning Summary:\n",
      "  Original rows: 7115\n",
      "  Cleaned rows: 7115\n",
      "  Rows removed: 0 (0.00%)\n",
      "  Data retained: 100.00%\n",
      "\n",
      " Missing Values AFTER Cleaning:\n",
      "Order Zipcode          6200\n",
      "Product Description    7115\n",
      "dtype: int64\n",
      "\n",
      "Late Delivery Risk Distribution:\n",
      "Late_delivery_risk\n",
      "1    4189\n",
      "0    2926\n",
      "Name: count, dtype: int64\n",
      "Late Risk Ratio: 58.88%\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================================\n",
    "# STEP 2: DATA CLEANING AND PREPROCESSING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"[STEP 2] Data Cleaning & Preprocessing...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Store original shape\n",
    "original_shape = df.shape\n",
    "\n",
    "# Check critical columns\n",
    "required_columns = ['Order City', 'Customer City', 'Late_delivery_risk']\n",
    "missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "\n",
    "print(\"\\n Handling Missing Values in Critical Columns...\")\n",
    "\n",
    "# Check missing values in critical columns\n",
    "for col in required_columns:\n",
    "    missing_count = df[col].isnull().sum()\n",
    "    missing_pct = (missing_count / len(df)) * 100\n",
    "    print(f\"  {col}: {missing_count} missing ({missing_pct:.2f}%)\")\n",
    "\n",
    "# Strategy 1: Drop rows with missing critical columns\n",
    "print(\"\\n Dropping rows with missing critical values...\")\n",
    "df_clean = df.dropna(subset=required_columns)\n",
    "print(f\"  Rows dropped: {len(df) - len(df_clean)}\")\n",
    "print(f\"  Remaining rows: {len(df_clean)}\")\n",
    "\n",
    "# Strategy 2: Clean city names (remove whitespace, handle case)\n",
    "print(\"\\n Cleaning city names...\")\n",
    "df_clean['Order City'] = df_clean['Order City'].str.strip().str.title()\n",
    "df_clean['Customer City'] = df_clean['Customer City'].str.strip().str.title()\n",
    "\n",
    "# Remove any rows where cities are empty strings after cleaning\n",
    "df_clean = df_clean[\n",
    "    (df_clean['Order City'].str.len() > 0) & \n",
    "    (df_clean['Customer City'].str.len() > 0)\n",
    "]\n",
    "print(f\"  After cleaning empty strings: {len(df_clean)} rows\")\n",
    "\n",
    "# Strategy 3: Handle Late_delivery_risk - ensure it's binary\n",
    "print(\"\\n  Validating Late_delivery_risk column...\")\n",
    "print(f\"  Unique values: {df_clean['Late_delivery_risk'].unique()}\")\n",
    "df_clean['Late_delivery_risk'] = df_clean['Late_delivery_risk'].astype(int)\n",
    "\n",
    "# Ensure values are 0 or 1\n",
    "invalid_risk = df_clean[~df_clean['Late_delivery_risk'].isin([0, 1])]\n",
    "if len(invalid_risk) > 0:\n",
    "    print(f\"    Warning: {len(invalid_risk)} rows with invalid risk values, removing...\")\n",
    "    df_clean = df_clean[df_clean['Late_delivery_risk'].isin([0, 1])]\n",
    "\n",
    "print(f\"  Valid risk values: {df_clean['Late_delivery_risk'].value_counts().to_dict()}\")\n",
    "\n",
    "# Strategy 4: Handle Shipping Mode (if exists)\n",
    "if 'Shipping Mode' in df_clean.columns:\n",
    "    print(\"\\n Handling Shipping Mode...\")\n",
    "    shipping_missing = df_clean['Shipping Mode'].isnull().sum()\n",
    "    print(f\"  Missing Shipping Mode: {shipping_missing}\")\n",
    "    if shipping_missing > 0:\n",
    "        # Fill with 'Unknown' or most common value\n",
    "        mode_value = df_clean['Shipping Mode'].mode()[0] if len(df_clean['Shipping Mode'].mode()) > 0 else 'Standard'\n",
    "        df_clean['Shipping Mode'].fillna(mode_value, inplace=True)\n",
    "        print(f\"  Filled with: {mode_value}\")\n",
    "\n",
    "# Strategy 5: Remove duplicate edges (same Order City -> Customer City)\n",
    "print(\"\\n Handling duplicate routes...\")\n",
    "duplicates_before = len(df_clean)\n",
    "# Keep the one with highest risk (conservative approach)\n",
    "df_clean = df_clean.sort_values('Late_delivery_risk', ascending=False).drop_duplicates(\n",
    "    subset=['Order City', 'Customer City'], keep='first'\n",
    ")\n",
    "duplicates_removed = duplicates_before - len(df_clean)\n",
    "print(f\"  Duplicate routes removed: {duplicates_removed}\")\n",
    "print(f\"  Remaining unique routes: {len(df_clean)}\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n Data Cleaning Summary:\")\n",
    "print(f\"  Original rows: {original_shape[0]}\")\n",
    "print(f\"  Cleaned rows: {len(df_clean)}\")\n",
    "print(f\"  Rows removed: {original_shape[0] - len(df_clean)} ({((original_shape[0] - len(df_clean))/original_shape[0]*100):.2f}%)\")\n",
    "print(f\"  Data retained: {(len(df_clean)/original_shape[0]*100):.2f}%\")\n",
    "\n",
    "# Check if we have enough data left\n",
    "if len(df_clean) < 100:\n",
    "    raise ValueError(f\" Too few rows remaining ({len(df_clean)}). Check data quality!\")\n",
    "\n",
    "print(f\"\\n Missing Values AFTER Cleaning:\")\n",
    "missing_after = df_clean.isnull().sum()\n",
    "if missing_after.sum() > 0:\n",
    "    print(missing_after[missing_after > 0])\n",
    "else:\n",
    "    print(\"  No missing values!\")\n",
    "\n",
    "print(f\"\\nLate Delivery Risk Distribution:\")\n",
    "print(df_clean['Late_delivery_risk'].value_counts())\n",
    "print(f\"Late Risk Ratio: {df_clean['Late_delivery_risk'].mean():.2%}\")\n",
    "\n",
    "# Update df to use cleaned version\n",
    "df = df_clean.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5c32e88-c711-42b0-a395-df26f7a29ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "[STEP 3] Building Graph from Order-Customer City Connections...\n",
      "======================================================================\n",
      "\n",
      "Total Unique Cities (Nodes): 2726\n",
      "Sample Cities: ['New Orleans', 'Kediri', 'Itapecerica Da Serra', 'Flushing', 'Dordrecht', 'Broken Hill', 'Lanxi', 'Kisangani', 'Teziutlán', 'Mudanjiang']\n",
      "\n",
      "Graph Statistics:\n",
      "  Nodes: 2726\n",
      "  Edges: 7115\n",
      "  Density: 0.0010\n",
      "  Is Weakly Connected: False\n",
      "\n",
      "Top 5 Cities by Degree Centrality:\n",
      "  Caguas: 0.5306 (In: 1446, Out: 0)\n",
      "  Los Angeles: 0.0752 (In: 148, Out: 57)\n",
      "  Chicago: 0.0723 (In: 168, Out: 29)\n",
      "  Brooklyn: 0.0635 (In: 173, Out: 0)\n",
      "  New York: 0.0385 (In: 105, Out: 0)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================================\n",
    "# STEP 3: BUILD GRAPH STRUCTURE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"[STEP 3] Building Graph from Order-Customer City Connections...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Identify unique cities\n",
    "all_cities = list(set(df['Order City'].tolist() + df['Customer City'].tolist()))\n",
    "num_nodes = len(all_cities)\n",
    "city_to_idx = {city: i for i, city in enumerate(all_cities)}\n",
    "idx_to_city = {i: city for city, i in city_to_idx.items()}\n",
    "\n",
    "print(f\"\\nTotal Unique Cities (Nodes): {num_nodes}\")\n",
    "print(f\"Sample Cities: {all_cities[:10]}\")\n",
    "\n",
    "# Build directed graph\n",
    "G = nx.DiGraph()\n",
    "edge_attributes = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    src = city_to_idx[row['Order City']]\n",
    "    dst = city_to_idx[row['Customer City']]\n",
    "    late_risk = row['Late_delivery_risk']\n",
    "    \n",
    "    # Add edge with attributes\n",
    "    if G.has_edge(src, dst):\n",
    "        # If edge exists, aggregate (take max risk)\n",
    "        G[src][dst]['late_risk'] = max(G[src][dst]['late_risk'], late_risk)\n",
    "        G[src][dst]['count'] += 1\n",
    "    else:\n",
    "        G.add_edge(src, dst, late_risk=late_risk, count=1)\n",
    "\n",
    "print(f\"\\nGraph Statistics:\")\n",
    "print(f\"  Nodes: {G.number_of_nodes()}\")\n",
    "print(f\"  Edges: {G.number_of_edges()}\")\n",
    "print(f\"  Density: {nx.density(G):.4f}\")\n",
    "print(f\"  Is Weakly Connected: {nx.is_weakly_connected(G)}\")\n",
    "\n",
    "# Identify isolated nodes\n",
    "isolated_nodes = list(nx.isolates(G))\n",
    "if len(isolated_nodes) > 0:\n",
    "    print(f\"  Isolated nodes: {len(isolated_nodes)}\")\n",
    "    print(f\"  Removing isolated nodes...\")\n",
    "    G.remove_nodes_from(isolated_nodes)\n",
    "    print(f\"  Nodes after removal: {G.number_of_nodes()}\")\n",
    "    num_nodes = G.number_of_nodes()\n",
    "    \n",
    "    # Update mappings\n",
    "    active_cities = [idx_to_city[i] for i in G.nodes()]\n",
    "    city_to_idx = {city: i for i, city in enumerate(active_cities)}\n",
    "    idx_to_city = {i: city for city, i in city_to_idx.items()}\n",
    "    \n",
    "    # Relabel graph\n",
    "    mapping = {old: new for new, old in enumerate(G.nodes())}\n",
    "    G = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "# Calculate network metrics\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "in_degree = dict(G.in_degree())\n",
    "out_degree = dict(G.out_degree())\n",
    "\n",
    "print(f\"\\nTop 5 Cities by Degree Centrality:\")\n",
    "top_cities = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "for idx, centrality in top_cities:\n",
    "    print(f\"  {idx_to_city[idx]}: {centrality:.4f} (In: {in_degree[idx]}, Out: {out_degree[idx]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "674ad0ce-e9e9-4e31-9582-acefff47d8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "[STEP 4] Creating Node Features...\n",
      "======================================================================\n",
      "\n",
      "[4.1] Feature Quality Check:\n",
      "  NaN values in features: 0\n",
      "  Inf values in features: 0\n",
      "\n",
      "Node Feature Matrix Shape: torch.Size([2726, 16])\n",
      "Feature Statistics:\n",
      "  Mean: 0.0868\n",
      "  Std: 0.1737\n",
      "  Min: 0.0000\n",
      "  Max: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================\n",
    "# STEP 4: CREATE NODE FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"[STEP 4] Creating Node Features...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize feature matrix\n",
    "feature_dim = 16\n",
    "features = np.zeros((num_nodes, feature_dim))\n",
    "\n",
    "# Feature engineering for each node\n",
    "for node in range(num_nodes):\n",
    "    # Feature 0-1: In/Out degree (normalized)\n",
    "    max_in = max(in_degree.values()) if in_degree.values() else 1\n",
    "    max_out = max(out_degree.values()) if out_degree.values() else 1\n",
    "    features[node, 0] = in_degree.get(node, 0) / max_in\n",
    "    features[node, 1] = out_degree.get(node, 0) / max_out\n",
    "    \n",
    "    # Feature 2: Degree centrality\n",
    "    features[node, 2] = degree_centrality.get(node, 0)\n",
    "    \n",
    "    # Feature 3-4: Risk statistics (incoming/outgoing edges)\n",
    "    incoming_risks = [G[u][node]['late_risk'] for u in G.predecessors(node)]\n",
    "    outgoing_risks = [G[node][v]['late_risk'] for v in G.successors(node)]\n",
    "    \n",
    "    features[node, 3] = np.mean(incoming_risks) if incoming_risks else 0\n",
    "    features[node, 4] = np.mean(outgoing_risks) if outgoing_risks else 0\n",
    "    \n",
    "    # Feature 5-6: Edge count statistics\n",
    "    features[node, 5] = len(incoming_risks) / num_nodes  # Normalized\n",
    "    features[node, 6] = len(outgoing_risks) / num_nodes  # Normalized\n",
    "    \n",
    "    # Features 7-15: Random embeddings (can be replaced with geographic/market features)\n",
    "    features[node, 7:] = np.abs(np.random.randn(feature_dim - 7)) * 0.1\n",
    "\n",
    "# Check for NaN or Inf in features\n",
    "print(f\"\\n[4.1] Feature Quality Check:\")\n",
    "nan_count = np.isnan(features).sum()\n",
    "inf_count = np.isinf(features).sum()\n",
    "print(f\"  NaN values in features: {nan_count}\")\n",
    "print(f\"  Inf values in features: {inf_count}\")\n",
    "\n",
    "if nan_count > 0 or inf_count > 0:\n",
    "    print(f\"   Replacing NaN/Inf with zeros...\")\n",
    "    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "features = torch.FloatTensor(features)\n",
    "print(f\"\\nNode Feature Matrix Shape: {features.shape}\")\n",
    "print(f\"Feature Statistics:\")\n",
    "print(f\"  Mean: {features.mean().item():.4f}\")\n",
    "print(f\"  Std: {features.std().item():.4f}\")\n",
    "print(f\"  Min: {features.min().item():.4f}\")\n",
    "print(f\"  Max: {features.max().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81a39d8b-62d1-44ee-bef1-52add7f70a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "[STEP 5] Creating Normalized Adjacency Matrix...\n",
      "======================================================================\n",
      "Adjacency Matrix Shape: (2726, 2726)\n",
      "Number of Connections: 7115\n",
      "After adding self-loops: 9836 connections\n",
      "\n",
      "[5.1] Adjacency Matrix Quality Check:\n",
      "  NaN values: 0\n",
      "  Inf values: 0\n",
      "\n",
      "Normalized Adjacency Matrix Shape: torch.Size([2726, 2726])\n",
      "Normalization check (row sums ≈ 1):\n",
      "  Min row sum: 1.0000\n",
      "  Max row sum: 1.0000\n",
      "  Mean row sum: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================================\n",
    "# STEP 5: CREATE NORMALIZED ADJACENCY MATRIX\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"[STEP 5] Creating Normalized Adjacency Matrix...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "adj = nx.to_numpy_array(G, nodelist=range(num_nodes))\n",
    "print(f\"Adjacency Matrix Shape: {adj.shape}\")\n",
    "print(f\"Number of Connections: {np.sum(adj > 0)}\")\n",
    "\n",
    "# Add self-loops for GNN stability\n",
    "adj = adj + np.eye(num_nodes)\n",
    "print(f\"After adding self-loops: {np.sum(adj > 0)} connections\")\n",
    "\n",
    "# Normalize by degree (D^-1 * A)\n",
    "degrees = np.sum(adj, axis=1)\n",
    "degrees[degrees == 0] = 1  # Avoid division by zero\n",
    "\n",
    "# Additional safety check\n",
    "if np.any(degrees == 0):\n",
    "    print(\"   Warning: Zero degrees found even after self-loops!\")\n",
    "\n",
    "adj_norm = adj / degrees[:, None]\n",
    "\n",
    "# Check for NaN or Inf in adjacency\n",
    "print(f\"\\n[5.1] Adjacency Matrix Quality Check:\")\n",
    "adj_nan = np.isnan(adj_norm).sum()\n",
    "adj_inf = np.isinf(adj_norm).sum()\n",
    "print(f\"  NaN values: {adj_nan}\")\n",
    "print(f\"  Inf values: {adj_inf}\")\n",
    "\n",
    "if adj_nan > 0 or adj_inf > 0:\n",
    "    print(f\"   Replacing NaN/Inf with zeros...\")\n",
    "    adj_norm = np.nan_to_num(adj_norm, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "adj_norm = torch.FloatTensor(adj_norm)\n",
    "\n",
    "print(f\"\\nNormalized Adjacency Matrix Shape: {adj_norm.shape}\")\n",
    "print(f\"Normalization check (row sums ≈ 1):\")\n",
    "print(f\"  Min row sum: {adj_norm.sum(dim=1).min().item():.4f}\")\n",
    "print(f\"  Max row sum: {adj_norm.sum(dim=1).max().item():.4f}\")\n",
    "print(f\"  Mean row sum: {adj_norm.sum(dim=1).mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bb11b17-fc12-4c5c-9273-6f22812e4663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "[STEP 6] Preparing Edge Training Data...\n",
      "======================================================================\n",
      "Total Edges: 7115\n",
      "Positive Labels (Late Risk=1): 4189 (58.88%)\n",
      "Negative Labels (Late Risk=0): 2926 (41.12%)\n",
      "\n",
      "Train Set: 5692 edges\n",
      "  Positive: 3351 (58.87%)\n",
      "Test Set: 1423 edges\n",
      "  Positive: 838 (58.89%)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================================\n",
    "# STEP 6: PREPARE TRAINING DATA (EDGES AND LABELS)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"[STEP 6] Preparing Edge Training Data...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "edges = [(u, v) for u, v in G.edges()]\n",
    "labels = [G[u][v]['late_risk'] for u, v in edges]\n",
    "\n",
    "print(f\"Total Edges: {len(edges)}\")\n",
    "print(f\"Positive Labels (Late Risk=1): {sum(labels)} ({sum(labels)/len(labels):.2%})\")\n",
    "print(f\"Negative Labels (Late Risk=0): {len(labels)-sum(labels)} ({(len(labels)-sum(labels))/len(labels):.2%})\")\n",
    "\n",
    "# Check for class imbalance\n",
    "risk_ratio = sum(labels) / len(labels)\n",
    "if risk_ratio < 0.1 or risk_ratio > 0.9:\n",
    "    print(f\"    Warning: Severe class imbalance detected ({risk_ratio:.2%})!\")\n",
    "    print(f\"  Consider using class weights in loss function.\")\n",
    "\n",
    "# Split data\n",
    "if len(edges) < 10:\n",
    "    raise ValueError(f\"Too few edges ({len(edges)}) for train/test split!\")\n",
    "\n",
    "edge_train, edge_test, label_train, label_test = train_test_split(\n",
    "    edges, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain Set: {len(edge_train)} edges\")\n",
    "print(f\"  Positive: {sum(label_train)} ({sum(label_train)/len(label_train):.2%})\")\n",
    "print(f\"Test Set: {len(edge_test)} edges\")\n",
    "print(f\"  Positive: {sum(label_test)} ({sum(label_test)/len(label_test):.2%})\")\n",
    "\n",
    "# Convert to tensors\n",
    "label_train_tensor = torch.FloatTensor(label_train)\n",
    "label_test_tensor = torch.FloatTensor(label_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "507cec5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "[STEP 7] Defining FIXED Graph Neural Network Architecture...\n",
      "======================================================================\n",
      "\n",
      " FIXED Model Architecture:\n",
      "SupplyChainGNN(\n",
      "  (gcn1): GCNLayer()\n",
      "  (gcn2): GCNLayer()\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (edge_predictor): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=16, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Total Parameters: 1857\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 7: DEFINE GNN MODEL - FIXED VERSION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"[STEP 7] Defining FIXED Graph Neural Network Architecture...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        support = torch.mm(x, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "        return F.relu(output)\n",
    "\n",
    "class SupplyChainGNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SupplyChainGNN, self).__init__()\n",
    "        self.gcn1 = GCNLayer(input_dim, hidden_dim)\n",
    "        self.gcn2 = GCNLayer(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        #  FIX: Add proper edge predictor MLP\n",
    "        self.edge_predictor = nn.Sequential(\n",
    "            nn.Linear(output_dim * 2, hidden_dim),  # Concatenate src + dst embeddings\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)  # Output single logit\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        x = self.gcn1(x, adj)\n",
    "        x = self.dropout(x)\n",
    "        x = self.gcn2(x, adj)\n",
    "        return x\n",
    "    \n",
    "    def predict_edge(self, embeds, src, dst):\n",
    "        \"\"\" FIXED: Use MLP for edge prediction instead of dot product\"\"\"\n",
    "        src_embed = embeds[src]\n",
    "        dst_embed = embeds[dst]\n",
    "        \n",
    "        # Concatenate source and destination embeddings\n",
    "        edge_repr = torch.cat([src_embed, dst_embed], dim=0)\n",
    "        \n",
    "        # Pass through MLP and apply sigmoid\n",
    "        logit = self.edge_predictor(edge_repr)\n",
    "        return torch.sigmoid(logit.squeeze())\n",
    "    \n",
    "    def predict_edges_batch(self, embeds, edge_list):\n",
    "        \"\"\" FIXED: Vectorized batch prediction with MLP\"\"\"\n",
    "        src_indices = torch.LongTensor([src for src, dst in edge_list])\n",
    "        dst_indices = torch.LongTensor([dst for src, dst in edge_list])\n",
    "        \n",
    "        src_embeds = embeds[src_indices]  # (num_edges, embed_dim)\n",
    "        dst_embeds = embeds[dst_indices]  # (num_edges, embed_dim)\n",
    "        \n",
    "        # Concatenate along feature dimension\n",
    "        edge_reprs = torch.cat([src_embeds, dst_embeds], dim=1)  # (num_edges, embed_dim*2)\n",
    "        \n",
    "        # Pass through MLP\n",
    "        logits = self.edge_predictor(edge_reprs)  # (num_edges, 1)\n",
    "        return torch.sigmoid(logits.squeeze())\n",
    "\n",
    "model = SupplyChainGNN(input_dim=16, hidden_dim=32, output_dim=8)\n",
    "print(f\"\\n FIXED Model Architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal Parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6f3d25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "[STEP 8] Training the FIXED Model...\n",
      "======================================================================\n",
      "\n",
      " FIXED Training Configuration:\n",
      "  Learning Rate: 0.005\n",
      "  Weight Decay: 1e-4\n",
      "  Epochs: 300\n",
      "  Dropout: 0.3\n",
      "  Class Weight (positive): 0.70\n",
      "  Using MLP Edge Predictor (no more saturation!)\n",
      "Epoch   0 | Train Loss: 0.4775 | Train Acc: 0.6557 | Test Loss: 0.5586 | Test Acc: 0.6732\n",
      "Epoch  20 | Train Loss: 0.4724 | Train Acc: 0.6537 | Test Loss: 0.5939 | Test Acc: 0.6304\n",
      "Epoch  40 | Train Loss: 0.4752 | Train Acc: 0.6667 | Test Loss: 0.5689 | Test Acc: 0.6578\n",
      "Epoch  60 | Train Loss: 0.4757 | Train Acc: 0.6636 | Test Loss: 0.5688 | Test Acc: 0.6535\n",
      "Epoch  80 | Train Loss: 0.4686 | Train Acc: 0.6636 | Test Loss: 0.5743 | Test Acc: 0.6486\n",
      "Epoch 100 | Train Loss: 0.4693 | Train Acc: 0.6528 | Test Loss: 0.5721 | Test Acc: 0.6465\n",
      "Epoch 120 | Train Loss: 0.4661 | Train Acc: 0.6609 | Test Loss: 0.5775 | Test Acc: 0.6409\n",
      "Epoch 140 | Train Loss: 0.4784 | Train Acc: 0.6600 | Test Loss: 0.5644 | Test Acc: 0.6550\n",
      "Epoch 160 | Train Loss: 0.4737 | Train Acc: 0.6528 | Test Loss: 0.5729 | Test Acc: 0.6451\n",
      "Epoch 180 | Train Loss: 0.4684 | Train Acc: 0.6688 | Test Loss: 0.5616 | Test Acc: 0.6669\n",
      "Epoch 200 | Train Loss: 0.4740 | Train Acc: 0.6657 | Test Loss: 0.5756 | Test Acc: 0.6472\n",
      "Epoch 220 | Train Loss: 0.4746 | Train Acc: 0.6502 | Test Loss: 0.5648 | Test Acc: 0.6514\n",
      "Epoch 240 | Train Loss: 0.4680 | Train Acc: 0.6488 | Test Loss: 0.5614 | Test Acc: 0.6627\n",
      "Epoch 260 | Train Loss: 0.4622 | Train Acc: 0.6641 | Test Loss: 0.5619 | Test Acc: 0.6550\n",
      "Epoch 280 | Train Loss: 0.4644 | Train Acc: 0.6692 | Test Loss: 0.5574 | Test Acc: 0.6683\n",
      "\n",
      " Training Complete! Time: 31.83s\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 8: TRAINING LOOP \n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"[STEP 8] Training the FIXED Model...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.005, weight_decay=1e-4)  # Lower LR for stability\n",
    "loss_fn = nn.BCELoss()\n",
    "num_epochs = 300\n",
    "\n",
    "#  \n",
    "pos_weight = (len(label_train) - sum(label_train)) / sum(label_train) if sum(label_train) > 0 else 1.0\n",
    "loss_fn_weighted = nn.BCELoss(reduction='none')\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "print(\"\\n FIXED Training Configuration:\")\n",
    "print(f\"  Learning Rate: 0.005\")\n",
    "print(f\"  Weight Decay: 1e-4\")\n",
    "print(f\"  Epochs: {num_epochs}\")\n",
    "print(f\"  Dropout: 0.3\")\n",
    "print(f\"  Class Weight (positive): {pos_weight:.2f}\")\n",
    "print(f\"  Using MLP Edge Predictor (no more saturation!)\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "best_test_loss = float('inf')\n",
    "patience = 50\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    embeds = model(features, adj_norm)\n",
    "    \n",
    "    if torch.isnan(embeds).any():\n",
    "        print(f\"    NaN detected in embeddings at epoch {epoch}!\")\n",
    "        break\n",
    "    \n",
    "    train_preds = model.predict_edges_batch(embeds, edge_train)\n",
    "    \n",
    "   \n",
    "    losses = loss_fn_weighted(train_preds, label_train_tensor)\n",
    "    weights = torch.where(label_train_tensor == 1, pos_weight, 1.0)\n",
    "    loss = (losses * weights).mean()\n",
    "    \n",
    "    # L2 regularization on embeddings\n",
    "    embedding_reg = 0.0001 * torch.norm(embeds, p=2)\n",
    "    loss = loss + embedding_reg\n",
    "    \n",
    "    if torch.isnan(loss):\n",
    "        print(f\"    NaN loss at epoch {epoch}!\")\n",
    "        break\n",
    "    \n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    train_preds_binary = (train_preds.detach() > 0.5).float()\n",
    "    train_acc = (train_preds_binary == label_train_tensor).float().mean().item()\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    if epoch % 20 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_embeds = model(features, adj_norm)\n",
    "            test_preds = model.predict_edges_batch(test_embeds, edge_test)\n",
    "            test_loss = loss_fn(test_preds, label_test_tensor).item()\n",
    "            test_losses.append(test_loss)\n",
    "            \n",
    "            test_preds_binary = (test_preds > 0.5).float()\n",
    "            test_acc = (test_preds_binary == label_test_tensor).float().mean().item()\n",
    "            test_accs.append(test_acc)\n",
    "            \n",
    "            print(f\"Epoch {epoch:3d} | Train Loss: {loss.item():.4f} | Train Acc: {train_acc:.4f} | Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if test_loss < best_test_loss:\n",
    "                best_test_loss = test_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\n   Early stopping at epoch {epoch} (no improvement for {patience} checks)\")\n",
    "                break\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\n Training Complete! Time: {training_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d2d724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "[STEP 9] Model Evaluation...\n",
      "======================================================================\n",
      "\n",
      " Test Set Performance:\n",
      "  Accuracy: 0.6613\n",
      "  AUC-ROC: 0.7552\n",
      "\n",
      " Prediction Distribution Analysis:\n",
      "  Min Prediction: 0.0044\n",
      "  Max Prediction: 0.9592\n",
      "  Mean Prediction: 0.5021\n",
      "  Std Prediction: 0.2080\n",
      "  Median Prediction: 0.4957\n",
      "\n",
      " Saturation Check (should be MUCH better now):\n",
      "  Predictions > 0.95: 14 (1.0%)\n",
      "  Predictions < 0.05: 68 (4.8%)\n",
      "  Well-calibrated (0.05-0.95): 1341 (94.2%)\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Risk       0.57      0.72      0.64       585\n",
      "   Late Risk       0.76      0.62      0.68       838\n",
      "\n",
      "    accuracy                           0.66      1423\n",
      "   macro avg       0.66      0.67      0.66      1423\n",
      "weighted avg       0.68      0.66      0.66      1423\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[420 165]\n",
      " [317 521]]\n",
      "\n",
      "Confusion Matrix Breakdown:\n",
      "  True Negatives (Correct No Risk): 420\n",
      "  False Positives (Predicted Risk, Actually No Risk): 165\n",
      "  False Negatives (Predicted No Risk, Actually Risk): 317\n",
      "  True Positives (Correct Late Risk): 521\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 9: EVALUATION \n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"[STEP 9] Model Evaluation...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    final_embeds = model(features, adj_norm)\n",
    "    test_preds_prob = model.predict_edges_batch(final_embeds, edge_test).numpy()\n",
    "    test_preds_binary = (test_preds_prob > 0.5).astype(int)\n",
    "\n",
    "acc = accuracy_score(label_test, test_preds_binary)\n",
    "auc = roc_auc_score(label_test, test_preds_prob)\n",
    "\n",
    "print(f\"\\n Test Set Performance:\")\n",
    "print(f\"  Accuracy: {acc:.4f}\")\n",
    "print(f\"  AUC-ROC: {auc:.4f}\")\n",
    "\n",
    "print(f\"\\n Prediction Distribution Analysis:\")\n",
    "print(f\"  Min Prediction: {test_preds_prob.min():.4f}\")\n",
    "print(f\"  Max Prediction: {test_preds_prob.max():.4f}\")\n",
    "print(f\"  Mean Prediction: {test_preds_prob.mean():.4f}\")\n",
    "print(f\"  Std Prediction: {test_preds_prob.std():.4f}\")\n",
    "print(f\"  Median Prediction: {np.median(test_preds_prob):.4f}\")\n",
    "\n",
    "saturated_high = (test_preds_prob > 0.95).sum()\n",
    "saturated_low = (test_preds_prob < 0.05).sum()\n",
    "print(f\"\\n Saturation Check (should be MUCH better now):\")\n",
    "print(f\"  Predictions > 0.95: {saturated_high} ({saturated_high/len(test_preds_prob)*100:.1f}%)\")\n",
    "print(f\"  Predictions < 0.05: {saturated_low} ({saturated_low/len(test_preds_prob)*100:.1f}%)\")\n",
    "print(f\"  Well-calibrated (0.05-0.95): {len(test_preds_prob)-saturated_high-saturated_low} ({(len(test_preds_prob)-saturated_high-saturated_low)/len(test_preds_prob)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(label_test, test_preds_binary, \n",
    "                          target_names=['No Risk', 'Late Risk'], zero_division=0))\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(label_test, test_preds_binary)\n",
    "print(cm)\n",
    "print(f\"\\nConfusion Matrix Breakdown:\")\n",
    "print(f\"  True Negatives (Correct No Risk): {cm[0,0]}\")\n",
    "print(f\"  False Positives (Predicted Risk, Actually No Risk): {cm[0,1]}\")\n",
    "print(f\"  False Negatives (Predicted No Risk, Actually Risk): {cm[1,0]}\")\n",
    "print(f\"  True Positives (Correct Late Risk): {cm[1,1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f49ca47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "[STEP 10] Supply Chain Risk Insights...\n",
      "======================================================================\n",
      "\n",
      "Top 10 High-Risk Cities (by embedding magnitude):\n",
      "  1. Lanxi: Risk=1.0439, In=0, Out=1\n",
      "  2. Acarigua: Risk=1.0082, In=0, Out=1\n",
      "  3. Yaroslavl: Risk=1.0029, In=0, Out=1\n",
      "  4. Halmstad: Risk=1.0028, In=0, Out=1\n",
      "  5. Beirut: Risk=0.9568, In=0, Out=1\n",
      "  6. Springdale: Risk=0.9388, In=0, Out=1\n",
      "  7. Puerto Montt: Risk=0.9114, In=0, Out=1\n",
      "  8. Woodland: Risk=0.8940, In=0, Out=1\n",
      "  9. Pitesti: Risk=0.8800, In=0, Out=1\n",
      "  10. Kitwe: Risk=0.8764, In=0, Out=1\n",
      "\n",
      "Top 10 Highest Risk Shipping Routes:\n",
      "  1. Belfort → Caguas: Predicted=0.9592, Actual=1 |\n",
      "  2. Abha → Caguas: Predicted=0.9576, Actual=1 |\n",
      "  3. Potenza → Caguas: Predicted=0.9569, Actual=1 |\n",
      "  4. Carquefou → Caguas: Predicted=0.9549, Actual=1 |\n",
      "  5. Arnhem → Caguas: Predicted=0.9548, Actual=1 |\n",
      "  6. Lagos De Moreno → Caguas: Predicted=0.9541, Actual=1 |\n",
      "  7. Lausanne → Caguas: Predicted=0.9532, Actual=1 |\n",
      "  8. Ajaccio → Caguas: Predicted=0.9531, Actual=1 |\n",
      "  9. Elazig → Caguas: Predicted=0.9527, Actual=1 |\n",
      "  10. Assen → Caguas: Predicted=0.9527, Actual=1 |\n"
     ]
    }
   ],
   "source": [
    "#= ===========================================================================\n",
    "# STEP 10: RISK ANALYSIS & INSIGHTS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"[STEP 10] Supply Chain Risk Insights...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "node_risk_scores = torch.norm(final_embeds, dim=1).numpy()\n",
    "high_risk_nodes = np.argsort(node_risk_scores)[-10:]\n",
    "\n",
    "print(f\"\\nTop 10 High-Risk Cities (by embedding magnitude):\")\n",
    "for rank, node_idx in enumerate(high_risk_nodes[::-1], 1):\n",
    "    city_name = idx_to_city[node_idx]\n",
    "    risk_score = node_risk_scores[node_idx]\n",
    "    in_deg = in_degree.get(node_idx, 0)\n",
    "    out_deg = out_degree.get(node_idx, 0)\n",
    "    print(f\"  {rank}. {city_name}: Risk={risk_score:.4f}, In={in_deg}, Out={out_deg}\")\n",
    "\n",
    "edge_risks = [(edge, prob) for edge, prob in zip(edge_test, test_preds_prob)]\n",
    "edge_risks_sorted = sorted(edge_risks, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\nTop 10 Highest Risk Shipping Routes:\")\n",
    "for rank, ((src, dst), prob) in enumerate(edge_risks_sorted[:10], 1):\n",
    "    src_city = idx_to_city[src]\n",
    "    dst_city = idx_to_city[dst]\n",
    "    actual = label_test[edge_test.index((src, dst))]\n",
    "    match =  \"|\"if (prob > 0.5 and actual == 1) or (prob <= 0.5 and actual == 0) else \"✗\"\n",
    "    print(f\"  {rank}. {src_city} → {dst_city}: Predicted={prob:.4f}, Actual={actual} {match}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
